{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMObIJMkTTmYwgGbfZDo0LK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-SAAD-BIN-MAZHAR/GenerativeAICourse/blob/main/HuggingFaceDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9FwZEghXSza"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n"
      ],
      "metadata": {
        "id": "X9k7ry1nXfJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NcxsPW8AXf45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-B4TTFgRXj_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRb05Nz9X4pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face Task**"
      ],
      "metadata": {
        "id": "p87O59J6Ysxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "#---------------------------------------------------#\n",
        "#                     NLP TASKS                     #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Text Classification: Assigning a category to a piece of text.\n",
        "Sentiment Analysis\n",
        "Topic Classification\n",
        "Spam Detection '''\n",
        "\n",
        "classifier = pipeline(\"text-classification\")\n",
        "\n",
        "'''\n",
        "2. Token Classification: Assigning labels to individual tokens in a sequence.\n",
        "Named Entity Recognition (NER)\n",
        "Part-of-Speech Tagging\n",
        "'''\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "\n",
        "'''\n",
        "3. Question Answering: Extracting an answer from a given context based on a question.\n",
        "'''\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "\n",
        "'''\n",
        "4. Text Generation: Generating text based on a given prompt.\n",
        "Language Modeling\n",
        "Story Generation\n",
        "\n",
        "'''\n",
        "\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "'''\n",
        "5. Summarization: Condensing long documents into shorter summaries.\n",
        "'''\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "'''\n",
        "Translation: Translating text from one language to another.\n",
        "'''\n",
        "\n",
        "translator = pipeline(\"translation\",\n",
        "                      model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "'''\n",
        "6. Text2Text Generation: General-purpose text transformation, including summarization and translation.\n",
        "'''\n",
        "\n",
        "text2text_generator = pipeline(\"text2text-generation\")\n",
        "\n",
        "'''\n",
        "7. Fill-Mask: Predicting the masked token in a sequence.\n",
        "'''\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\")\n",
        "\n",
        "'''\n",
        "8. Feature Extraction: Extracting hidden states or features from text.\n",
        "'''\n",
        "\n",
        "feature_extractor = pipeline(\"feature-extraction\")\n",
        "\n",
        "'''\n",
        "9. Sentence Similarity: Measuring the similarity between two sentences.\n",
        "'''\n",
        "sentence_similarity = pipeline(\"sentence-similarity\")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#             Computer Vision TASKS                 #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image Classification: Classifying the main content of an image.\n",
        "\n",
        "'''\n",
        "\n",
        "image_classifier = pipeline(\"image-classification\")\n",
        "\n",
        "'''\n",
        "2. Object Detection: Identifying objects within an image and their bounding boxes.\n",
        "'''\n",
        "\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "\n",
        "'''\n",
        "3. Image Segmentation: Segmenting different parts of an image into classes.\n",
        "'''\n",
        "\n",
        "image_segmenter = pipeline(\"image-segmentation\")\n",
        "\n",
        "'''\n",
        "4. Image Generation: Generating images from textual descriptions (using DALL-E or similar models).\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#             Speech Processing TASKS               #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. utomatic Speech Recognition (ASR): Converting spoken language into text.\n",
        "'''\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "'''\n",
        "2. Speech Translation: Translating spoken language from one language to another.\n",
        "3. Audio Classification: Classifying audio signals into predefined categories.\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#                   Multimodal TASKS                #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image Captioning: Generating a textual description of an image.\n",
        "'''\n",
        "image_captioner = pipeline(\"image-to-text\")\n",
        "'''\n",
        "2. Visual Question Answering (VQA): Answering questions about the content of an image.\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#                     Other TASKS                   #\n",
        "#---------------------------------------------------#\n",
        "'''\n",
        "1. Table Question Answering: Answering questions based on tabular data.\n",
        "'''\n",
        "table_qa = pipeline(\"table-question-answering\")\n",
        "\n",
        "'''\n",
        "2. Document Question Answering: Extracting answers from documents like PDFs.\n",
        "\n",
        "'''\n",
        "doc_qa = pipeline(\"document-question-answering\")\n",
        "'''\n",
        "3. Time Series Forecasting: Predicting future values in time series data (not directly supported in the main Transformers library but available through extensions).\n",
        "'''"
      ],
      "metadata": {
        "id": "dGbOGURnYzr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg35Iur-Y2S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP TASK**"
      ],
      "metadata": {
        "id": "0eKd5RjgY4cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis**\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jC-dfQ5ha4ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier=pipeline('sentiment-analysis')\n",
        "\n",
        "result=classifier(\"I love this movie\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Fb6_gomvY7uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\")(\"I was confused with the Barbie Movie\")"
      ],
      "metadata": {
        "id": "3LzU2Ww2ZTAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\")\\\n",
        "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
        "                                      Lots of them Looks very Promising. \\\n",
        "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
        "                                      There is still lots to do.\\\n",
        "                                      Don't you think?\")"
      ],
      "metadata": {
        "id": "VY5r-du0aPUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\\\n",
        "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
        "                                      Lots of them Looks very Promising. \\\n",
        "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
        "                                      There is still lots to do.\\\n",
        "                                      Don't you think?\")\n"
      ],
      "metadata": {
        "id": "i16ZTiBxamcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Acy2AiTDasCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Batch Sentiment Analysis**"
      ],
      "metadata": {
        "id": "pmkiylFTbCo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
        "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
        "            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know.\",\\\n",
        "            \"I hate long Meetings.\"]\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "ey0UZdz5bIOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\", model = \"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
        "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
        "            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know. It is pretty funny name for a Regression Model.\",\\\n",
        "            \"I hate long Meetings.\"]\n",
        "classifier(task_list)"
      ],
      "metadata": {
        "id": "QjKqwO25bMd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCfDdyNwbQs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation**"
      ],
      "metadata": {
        "id": "EIarKLcjbSs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
        "generated_text = text_generator(\"Today is a rainy day in London\",\n",
        "                                truncation=True,\n",
        "                                num_return_sequences = 2)\n",
        "print(\"Generated_text:\\n \", generated_text[0]['generated_text'])"
      ],
      "metadata": {
        "id": "N0NwiDCdbVvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPo9yfp-bsD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question Anwsering**"
      ],
      "metadata": {
        "id": "UWxIGR86cG-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"What is my job?\"\n",
        "context = \"I am developing AI models with Python.\"\n",
        "qa_model(question = question, context = context)"
      ],
      "metadata": {
        "id": "AewvFUN-cL3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ZOY5V31cmqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "evlEKcpGcwyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification,DistilBertTokenizer,DistilBertForSequenceClassification\n"
      ],
      "metadata": {
        "id": "tj7vOfVjc0sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "mymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
        "mytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model = mymodel2 , tokenizer = mytokenizer2)\n",
        "res = classifier(\"I was so not happy with the Barbie Movie\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "RdJW7yfvdPaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Example text\n",
        "text = \"I was so not` happy with the## Barbie Movie?\"\n",
        "tokens=tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "5EakpV2PeFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokens to input ids\n",
        "input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "G2Lt51IJfFRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode the text (tokenization + converting to input IDs)\n",
        "encoded_input = tokenizer(text)\n",
        "print(\"Encoded Input:\", encoded_input)"
      ],
      "metadata": {
        "id": "ejuyBcj0fbUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decode the text\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)\n"
      ],
      "metadata": {
        "id": "ef1i8lSMfnyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Example text\n",
        "text = \"I was so not happy with the Barbie Movie\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to input IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "# Encode the text (tokenization + converting to input IDs)\n",
        "encoded_input = tokenizer(text)\n",
        "print(\"Encoded Input:\", encoded_input)\n",
        "\n",
        "# Decode the text\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)"
      ],
      "metadata": {
        "id": "p8oT5RGJfye7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2NunUYzgkea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @title Default title text\n",
        "**token_type_ids**<br>\n",
        "These IDs are used to distinguish between different sequences in tasks that involve multiple sentences, such as question-answering and sentence-pair classification. BERT uses this mechanism to understand which tokens belong to which segment. For single-sequence tasks like sentiment analysis, token_type_ids are all zeros.\n",
        "\n",
        "**attention_mask** <br>\n",
        "The attention mask is used to differentiate between actual tokens and padding tokens (if any). It helps the model focus on non-padding tokens and ignore padding tokens. A value of 1 indicates that the token should be attended to, while a value of 0 indicates padding.\n",
        "\n",
        "**Why Padding Tokens Are Used**<br>\n",
        "Uniform Sequence Length: Deep learning models typically process input data in batches. To efficiently process these batches, all sequences in a batch must have the same length. Padding tokens ensure this by extending shorter sequences to match the length of the longest sequence in the batch.\n",
        "Efficient Computation: Fixed-length sequences allow for more efficient use of hardware resources, as the model can process all sequences in parallel without needing to handle variable-length sequences individually.\n",
        "\n"
      ],
      "metadata": {
        "id": "wgwXoom8gsY1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUtSLPqNgsyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tunning IMDB**"
      ],
      "metadata": {
        "id": "70PSiOVPgwIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "qH8cvzeSgzdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrlY0P81g2qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and Prepare the Dataset"
      ],
      "metadata": {
        "id": "nq4H7DQJg8yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "UaK7JWGyg-o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "sMHgwpaShBEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "3W3LnFuBhB78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PehbOU10hLxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP-3 Preprocessing the Data**\n",
        "\n",
        "Tokenize the dataset using the tokenizer associated with the pre-trained model."
      ],
      "metadata": {
        "id": "YQYTn4pyhQLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "jQqOLnQThXJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "dukX9AwFiLiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "7qNRnt3miZPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Set Up the Training Arguments\n",
        "Specify the hyperparameters and training settings."
      ],
      "metadata": {
        "id": "AhXmhxE_iirR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    eval_strategy =\"epoch\",     # Evaluate every epoch\n",
        "    learning_rate=2e-5,              # Learning rate\n",
        "    per_device_train_batch_size=16,  # Batch size for training\n",
        "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
        "    num_train_epochs=1,              # Number of training epochs\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "xBBifEy8ijLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufUZbH2cjpMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Initialize the Model\n",
        "Load the pre-trained model and define the training procedure."
      ],
      "metadata": {
        "id": "Ka3HOHG8jxHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test']\n",
        ")\n"
      ],
      "metadata": {
        "id": "DQKxz7tyjxyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-eZF_c6kdTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train the Model\n",
        "Fine-tune the pre-trained model on your specific dataset.\n"
      ],
      "metadata": {
        "id": "SmYAZgEckhHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "rDB38Gjikiir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Evaluate the Model\n",
        "Assess the model's performance on a validation set."
      ],
      "metadata": {
        "id": "1TqC69r4lXWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "u9Na0yQCkrND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Save the Fine-Tuned Model\n",
        "Save the fine-tuned model for later use."
      ],
      "metadata": {
        "id": "isKiqAf_lrOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained('./fine-tuned-model')\n",
        "tokenizer.save_pretrained('./fine-tuned-tokenizer')\n"
      ],
      "metadata": {
        "id": "ni3x8x6wlsIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DP_pATB3lwZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c58gwfn0lwcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArXiv Project"
      ],
      "metadata": {
        "id": "eETLyHfDl5Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "TioTh2TLl5qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8kCY3OABl7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI-related papers\n",
        "query = 'ai OR artificial intelligence OR machine learning'\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "# Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "      'published': result.published,\n",
        "        'title': result.title,\n",
        "        'abstract': result.summary,\n",
        "        'categories': result.categories\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "OMwqo7szl_F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example abstract from API\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "id": "i2VxuuP4mUH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0]['summary_text']"
      ],
      "metadata": {
        "id": "Rjs2sohTmbFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jd-f15r_mrr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}